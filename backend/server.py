from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from openai import OpenAI
import os, signal, json, re, requests, urllib.parse, time
from datetime import datetime
from dotenv import load_dotenv

# ======================================================
# ‚öôÔ∏è Feature flags ‚Äî activables/d√©sactivables sans casser
# ======================================================
ENABLE_SYNTHESIS = True       # Ajoute une synth√®se narrative lisible
ENABLE_CONTEXT_BOX = True     # Ajoute un √©clairage contextuel court + web enrichi
ENABLE_TRANSPARENCY = True    # Ajoute mentions "exp√©rimental" et tronquage
ENABLE_URL_EXTRACT = True     # Active Trafilatura (si URL fournie)

# ======================================================
# Flask setup
# ======================================================
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}}, supports_credentials=True)

# ======================================================
# OpenAI client
# ======================================================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ======================================================
# Timeout (Render/Replit safety)
# ======================================================
def _timeout_handler(signum, frame):
    raise TimeoutError("Analyse trop longue (timeout Render).")
signal.signal(signal.SIGALRM, _timeout_handler)

# ======================================================
# Helpers
# ======================================================
def color_for(score: int) -> str:
    if score is None: return "‚ö™"
    if score >= 70: return "üü¢"
    if score >= 40: return "üü°"
    return "üî¥"

# ======================================================
# üåê Recherche Google CSE (Programmable Search API)
# ======================================================
ALLOWED_SITES = [
    "reuters.com", "apnews.com", "bbc.com",
    "lemonde.fr", "francetvinfo.fr",
    "lefigaro.fr", "liberation.fr", "leparisien.fr"
]

def search_web_results(queries, per_query=5, pause=0.5):
    """Recherche Google CSE (Programmable Search API) sur plusieurs entit√©s ou requ√™tes."""
    api_key = os.getenv("GOOGLE_CSE_API_KEY")
    cx = os.getenv("GOOGLE_CSE_CX")
    if not api_key or not cx:
        print("‚ö†Ô∏è GOOGLE_CSE_API_KEY ou GOOGLE_CSE_CX manquant ‚Äî recherche d√©sactiv√©e.")
        return []

    all_hits = []
    seen = set()
    for q in queries:
        site_filter = " OR ".join([f"site:{s}" for s in ALLOWED_SITES])
        full_q = f"{q} ({site_filter})"
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": cx,
            "q": full_q,
            "num": per_query,
            "hl": "fr",
            "lr": "lang_fr",
            "safe": "off"
        }
        try:
            r = requests.get(url, params=params, timeout=10)
            if r.status_code != 200:
                print("‚ö†Ô∏è Erreur Google CSE:", r.status_code, r.text[:200])
                continue
            data = r.json()
            results = []
            for item in data.get("items", []) or []:
                link = item.get("link")
                if link and link not in seen:
                    seen.add(link)
                    results.append({
                        "titre": item.get("title"),
                        "snippet": item.get("snippet"),
                        "url": link
                    })
            if results:
                all_hits.append({"entit√©": q, "sources": results})
            time.sleep(pause)
        except Exception as e:
            print("‚ö†Ô∏è Erreur recherche Google:", e)
            continue
    return all_hits

# ======================================================
# üß© Route principale : analyse
# ======================================================
@app.route("/analyze", methods=["POST", "OPTIONS"])
def analyze():
    if request.method == "OPTIONS":
        return ("", 204)

    payload = request.get_json(silent=True) or {}
    text = (payload.get("text") or "").strip()
    if not text:
        return jsonify({"error": "Aucun texte re√ßu"}), 400

    # üîó Extraction d‚ÄôURL via Trafilatura (si activ√©e)
    if ENABLE_URL_EXTRACT and re.match(r"^https?://", text):
        try:
            import trafilatura
            fetched = trafilatura.extract(trafilatura.fetch_url(text)) or ""
            if len(fetched.strip()) >= 300:
                text = fetched.strip()[:8000]
                print(f"‚úÖ Trafilatura OK (len={len(text)})")
            else:
                print("‚ö†Ô∏è Extraction trop courte, texte brut conserv√©.")
        except Exception as e:
            print("‚ö†Ô∏è Trafilatura indisponible :", e)

    # Tronquage protecteur
    MAX_LEN = 8000
    texte_tronque = len(text) > MAX_LEN
    original_length = len(text)
    if texte_tronque:
        text = text[:MAX_LEN] + " [...] (texte tronqu√© pour analyse)"

    # ======================================================
    # üß© √âtape 1 ‚Äî Pr√©-analyse de type de texte (faits/opinions/autres)
    # ======================================================
    try:
        pre_prompt = f"""
        Classe le texte selon 3 cat√©gories :
        - FAITS (affirmations v√©rifiables)
        - OPINIONS (jugements ou interpr√©tations)
        - AUTRES (ironie, satire, po√©sie, r√©cit, etc.)

        Retourne un JSON au format :
        {{
          "faits": <int>,
          "opinions": <int>,
          "autres": <int>
        }}
        Texte :
        {text[:2000]}
        """
        pre_resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Tu es un linguiste qui cat√©gorise les phrases d'un texte."},
                {"role": "user", "content": pre_prompt}
            ],
            temperature=0
        )
        fact_mix = json.loads(pre_resp.choices[0].message.content.strip())
    except Exception as e:
        print("‚ö†Ô∏è Erreur pr√©-analyse :", e)
        fact_mix = {"faits": 0, "opinions": 0, "autres": 0}

    total = sum(fact_mix.values()) or 1
    densite_faits = int((fact_mix["faits"] / total) * 100)
    type_texte = (
        "Principalement factuel" if densite_faits > 60 else
        "Opinion ou analyse" if fact_mix["opinions"] > 40 else
        "Autre (narratif, satirique‚Ä¶)"
    )

    # ======================================================
    # üåç √âtape interm√©diaire : Recherche Web enrichie
    #    (entit√©s ‚Üí recherche ‚Üí faits manquants / contradictions / impact)
    # ======================================================
    def web_context_research(text: str):
        """
        √âtape d'enrichissement factuel :
        1) Extrait les entit√©s du texte (personnes, lieux, orga, √©v√©nements)
        2) Recherche des sources fiables (Reuters, AP, BBC, Le Monde, Franceinfo)
        3) Synth√©tise : faits manquants pr√©cis + contradictions + impact + fiabilit√©
        Retour JSON robuste m√™me en cas d'√©chec partiel.
        """
        try:
            # 1) Extraction d'entit√©s
            ent_prompt = f"""
            Extrait les principales entit√©s nomm√©es (personnes, lieux, organisations, √©v√©nements, lois, chiffres cl√©s)
            du texte suivant :
            {text[:2000]}

            R√©ponds uniquement en JSON : ["entit√©1", "entit√©2", ...]
            """
            ent_resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu es un extracteur d'entit√©s journalistiques (NER)."},
                    {"role": "user", "content": ent_prompt}
                ],
                temperature=0
            )
            try:
                entities = json.loads(ent_resp.choices[0].message.content.strip())
            except Exception:
                entities = []
            if not isinstance(entities, list):
                entities = []
            entities = [e for e in entities if isinstance(e, str) and e.strip()]
            if not entities:
                return {
                    "recherches_effectuees": [],
                    "faits_manquants": [],
                    "contradictions": [],
                    "impact": "faible",
                    "fiabilite_sources": "Aucune source consultable (pas d'entit√©s d√©tect√©es).",
                    "synthese": "Aucune entit√© d√©tect√©e ‚Äî enrichissement impossible."
                }

            # 2) Recherche Web (Google CSE) ‚Äî requ√™tes multi-angles
            queries = []
            for ent in entities[:5]:
                queries += [
                    f"{ent} actualit√©",
                    f"{ent} controverse",
                    f"{ent} critiques",
                    f"{ent} biographie",
                    f"{ent} politique"
                ]
            recherches = search_web_results(queries, per_query=4)

            # 3) Fusion IA : comparer texte vs r√©sultats
            synth_prompt = f"""
            Compare le texte suivant :
            {text[:3500]}

            Avec ces r√©sultats de recherche (m√©dias g√©n√©ralistes fiables et agences) :
            {json.dumps(recherches, ensure_ascii=False, indent=2)}

            Ton r√¥le :
            1. Identifier les **faits pr√©cis manquants** (dates, chiffres, citations, critiques, d√©cisions officielles) √† ajouter.
            2. Signaler les **contradictions** ou corrections notables entre le texte et les sources.
            3. √âvaluer la **fiabilit√©** globale des sources (diversit√©, r√©putation).
            4. Estimer l‚Äô**impact** des manques/contradictions sur la compr√©hension du lecteur (faible / moyen / fort).
            5. R√©sumer en 2 phrases utiles.

            R√©ponds en JSON strict :
            {{
              "faits_manquants": [
                {{"texte": "<fait ajout√©>", "source": "<m√©dia>", "url": "<url ou null>"}}
              ],
              "contradictions": ["<phrase>", "..."],
              "impact": "<faible|moyen|fort>",
              "fiabilite_sources": "<phrase br√®ve>",
              "synthese": "<2 phrases de r√©sum√©>"
            }}
            """
            synth_resp = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Tu es un fact-checker journalistique expert et neutre."},
                    {"role": "user", "content": synth_prompt}
                ],
                temperature=0.3
            )

            try:
                result = json.loads(synth_resp.choices[0].message.content.strip())
            except Exception:
                # fallback compact si parsing impossible
                result = {
                    "faits_manquants": [],
                    "contradictions": [],
                    "impact": "faible",
                    "fiabilite_sources": "Synth√®se non structur√©e.",
                    "synthese": "La synth√®se n'a pas pu √™tre structur√©e."
                }

            # joindre les recherches brutes pour le front / debug
            result["recherches_effectuees"] = recherches
            # normaliser impact
            impact = (result.get("impact") or "faible").strip().lower()
            if impact not in ("faible", "moyen", "fort"):
                impact = "faible"
            result["impact"] = impact

            # nettoyer faits_manquants (format stable)
            fm = []
            for f in result.get("faits_manquants", []) or []:
                if isinstance(f, dict) and f.get("texte"):
                    fm.append({
                        "texte": str(f.get("texte")).strip(),
                        "source": (f.get("source") or "").strip() or None,
                        "url": (f.get("url") or "").strip() or None
                    })
            result["faits_manquants"] = fm

            # contradictions => liste de str
            contr = []
            for c in result.get("contradictions", []) or []:
                if isinstance(c, str) and c.strip():
                    contr.append(c.strip())
            result["contradictions"] = contr

            return result

        except Exception as e:
            print("‚ö†Ô∏è Web context failed:", e)
            return {
                "recherches_effectuees": [],
                "faits_manquants": [],
                "contradictions": [],
                "impact": "faible",
                "fiabilite_sources": "Recherche contextuelle non disponible.",
                "synthese": "Recherche contextuelle non disponible."
            }

    web_info = web_context_research(text) if ENABLE_CONTEXT_BOX else {
        "recherches_effectuees": [],
        "faits_manquants": [],
        "contradictions": [],
        "impact": "faible",
        "fiabilite_sources": "Contexte non activ√©.",
        "synthese": "Contexte non activ√©."
    }

    # ======================================================
    # üß† √âtape 3 ‚Äî Analyse principale compl√®te
    # ======================================================
    prompt = f"""
    Tu es **De Facto**, un analyste de contenu journalistique.  
    Ton r√¥le est d'√©valuer un texte selon deux axes : **FOND** (justesse, compl√©tude) et **FORME** (ton, sophismes),  
    puis de produire une **analyse claire, utile et concr√®te**.

    ---

    ### üéØ Objectif
    Fournir une **analyse journalistique enrichissante**, pas une √©valuation scolaire.  
    Chaque r√©ponse doit **aider l'utilisateur √† comprendre ce que le texte dit, oublie, ou oriente.**

    ---

    ### üß© Structure de sortie (STRICT JSON)
    Tu r√©pondras **uniquement** en JSON au format suivant :
    {{
      "score_global": <int>,
      "couleur_global": "<emoji>",
      "axes": {{
        "fond": {{
          "justesse": {{"note": <int>, "couleur": "<emoji>", "justification": "<phrase claire>", "citation": "<<=20 mots ou null>"}},
          "completude": {{"note": <int>, "couleur": "<emoji>", "justification": "<phrase claire>", "citation": "<<=20 mots ou null>"}}
        }},
        "forme": {{
          "ton": {{"note": <int>, "couleur": "<emoji>", "justification": "<phrase claire>", "citation": "<<=20 mots ou null>"}},
          "sophismes": {{"note": <int>, "couleur": "<emoji>", "justification": "<phrase claire>", "citation": "<<=20 mots ou null>"}}
        }}
      }},
      "commentaire": "<2 phrases de synth√®se journalistique>",
      "resume": "<3 phrases synth√©tiques, utiles et percutantes>",
      "confiance_analyse": <int>,
      "explication_confiance": "<phrase expliquant pourquoi la confiance est √† ce niveau>",
      "hypothese_interpretative": "<1 phrase : raison possible du ton ou du cadrage m√©diatique>",
      "limites_analyse_ia": ["<texte>", "..."],
      "limites_analyse_contenu": ["<texte>", "..."],
      "recherches_effectuees": ["<r√©sum√© court>", "..."],
      "methode": {{
        "principe": "De Facto √©value le texte selon deux axes : FOND (justesse, compl√©tude) et FORME (ton, sophismes).",
        "criteres": {{
          "fond": "Justesse (v√©racit√©/sources) et compl√©tude (pluralit√©/contre-arguments).",
          "forme": "Ton (neutralit√© lexicale) et sophismes (raisonnements fallacieux)."
        }},
        "avertissement": "Analyse exp√©rimentale ‚Äî le mod√®le peut commettre des erreurs."
      }}
    }}

    ---

    ### üß† Directives pour chaque section

    #### üü© Synth√®se globale (commentaire + r√©sum√©)
    R√©dige comme un mini article.  
    Mets en avant **ce qui manque, ce qui biaise, ou ce qui change la compr√©hension**.

    **Exemples :**
    - ¬´ L‚Äôarticle pr√©sente les faits judiciaires de mani√®re exacte mais omet les arguments de la d√©fense, ce qui oriente la lecture. ¬ª
    - ¬´ Le texte d√©crit l‚Äô√©motion du public sans rappeler les faits de base, cr√©ant une impression partielle. ¬ª
    - ¬´ Les donn√©es chiffr√©es sont exactes mais d√©contextualis√©es, ce qui exag√®re la gravit√© du ph√©nom√®ne. ¬ª

    √Ä √©viter :
    - ¬´ Le ton est neutre. ¬ª
    - ¬´ Le texte manque de d√©tails. ¬ª

    ---

    #### üß© D√©tails des 4 crit√®res

    **Exemples de bonnes justifications :**
    - Justesse üü¢ : ¬´ L‚Äôauteur cite la condamnation de 2021 avec pr√©cision. ¬ª
    - Compl√©tude üü° : ¬´ Aucune mention des arguments adverses. ¬ª
    - Ton üî¥ : ¬´ L‚Äôexpression ‚Äúenfin condamn√©‚Äù montre un parti pris implicite. ¬ª
    - Sophismes üü° : ¬´ L‚Äôauteur g√©n√©ralise √† partir d‚Äôun seul t√©moignage. ¬ª

    ### üì∞ Conscience du m√©dia
    Si le texte provient d‚Äôun m√©dia connu, identifie son orientation ou ton √©ditorial habituel
    et explique si cela peut influencer la pr√©sentation des faits.

    ---

    ### üåç Compl√©ments factuels trouv√©s sur le Web (√† exploiter)
    {json.dumps(web_info, ensure_ascii=False, indent=2)}

    ---
### ‚öîÔ∏è Instruction sp√©ciale ‚Äî mode "analyse investigatrice"
Utilise les r√©sultats de la recherche Web pour :
- Citer les faits pr√©cis absents du texte, avec leurs sources.
- √âvaluer la gravit√© de ces omissions : si elles changent la compr√©hension globale, abaisse fortement la note de compl√©tude.
- Si une contradiction claire est trouv√©e, baisse la note de justesse.
- Mentionne ces faits manquants explicitement dans le commentaire et le r√©sum√©.

    ### üßæ Texte √† analyser :
    ---
    {text}
    ---
    """

    try:
        signal.alarm(45)
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Tu es un analyste journalistique rigoureux, concis et clair."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.25
        )
        signal.alarm(0)

        raw = resp.choices[0].message.content.strip()
        try:
            result = json.loads(raw)
        except json.JSONDecodeError:
            m = re.search(r"\{.*\}", raw, re.DOTALL)
            if not m:
                return jsonify({"error": "R√©ponse GPT non conforme (non JSON)"}), 500
            result = json.loads(m.group(0))

        # ======================================================
        # Post-traitement enrichi
        # ======================================================
        result.setdefault("confiance_analyse", 80)
        result.setdefault("type_texte", type_texte)
        result.setdefault("densite_faits", densite_faits)

        # üéØ Pond√©ration douce du score global selon densit√© factuelle
        if "score_global" in result:
            sg = int(result["score_global"])
            if densite_faits > 60:
                sg = min(sg + 5, 100)
            elif densite_faits < 30:
                sg = max(sg - 5, 0)
            result["score_global"] = sg
            result["couleur_global"] = color_for(sg)

        # üîé Ajoute info sur le contenu du texte
        result["composition"] = {
            "faits": fact_mix["faits"],
            "opinions": fact_mix["opinions"],
            "autres": fact_mix["autres"],
            "densite_faits": densite_faits
        }

        # üåç Ins√®re le contexte Web brute pour le front
        result["faits_complementaires"] = web_info.get("faits_manquants", [])
        result["contexte_synthese"] = web_info.get("synthese")
        result["contexte_impact"] = web_info.get("impact")
        result["contexte_contradictions"] = web_info.get("contradictions", [])
        result["contexte_fiabilite_sources"] = web_info.get("fiabilite_sources", "")
        result["recherches_effectuees"] = web_info.get("recherches_effectuees", [])

        # üßÆ POND√âRATION INTELLIGENTE SELON LE CONTEXTE WEB
        # Bar√®me explicite :
        # - Contradictions : -20 (‚â•2), -10 (1)
        # - Faits manquants : -25 (‚â•3), -15 (2), -8 (1) sur compl√©tude
        # - Impact global "moyen" : -5 sur score global ; "fort" : -10
        axes = result.get("axes", {}) or {}
        fond = axes.get("fond", {}) or {}
        justesse = (fond.get("justesse", {}) or {}).get("note", 70)
        completude = (fond.get("completude", {}) or {}).get("note", 70)

        nb_contrad = len(web_info.get("contradictions", []) or [])
        nb_faits = len(web_info.get("faits_manquants", []) or [])
        impact = (web_info.get("impact") or "faible").lower()

        # Ajustements justesse par contradictions
        if nb_contrad >= 2:
            justesse -= 20
        elif nb_contrad == 1:
            justesse -= 10

        # Ajustements compl√©tude par faits manquants
        if nb_faits >= 3:
            completude -= 25
        elif nb_faits == 2:
            completude -= 15
        elif nb_faits == 1:
            completude -= 8

        # Clamps 0..100
        justesse = max(0, min(100, int(justesse)))
        completude = max(0, min(100, int(completude)))

        # Replace in result if structure exists
        if "fond" in axes:
            if "justesse" in axes["fond"]:
                axes["fond"]["justesse"]["note"] = justesse
                axes["fond"]["justesse"]["couleur"] = color_for(justesse)
            if "completude" in axes["fond"]:
                axes["fond"]["completude"]["note"] = completude
                axes["fond"]["completude"]["couleur"] = color_for(completude)

        # Ajustement score global par impact
        if "score_global" in result:
            if "fort" in impact:
                result["score_global"] = max(0, result["score_global"] - 10)
            elif "moyen" in impact:
                result["score_global"] = max(0, result["score_global"] - 5)
            result["couleur_global"] = color_for(result["score_global"])

        # ‚úÖ (Optionnel) Enregistrer une trace pour /logs
        try:
            log_item = {
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "input_len": original_length,
                "texte_tronque": bool(texte_tronque),
                "type_texte": type_texte,
                "densite_faits": densite_faits,
                "web_faits_manquants": nb_faits,
                "web_contradictions": nb_contrad,
                "web_impact": impact,
                "score_global": result.get("score_global"),
                "axes": result.get("axes", {}),
                "resume": result.get("resume"),
                "commentaire": result.get("commentaire"),
            }
            with open("logs.jsonl", "a", encoding="utf-8") as f:
                f.write(json.dumps(log_item, ensure_ascii=False) + "\n")
        except Exception as e:
            print("‚ÑπÔ∏è √âchec d'√©criture logs.jsonl :", e)

        return jsonify(result)

    except TimeoutError:
        return jsonify({"error": "Analyse trop longue. R√©essaie avec un texte plus court."}), 500
    except Exception as e:
        print("‚ùå Erreur :", e)
        return jsonify({"error": str(e)}), 500


# ======================================================
# üìú Historique des analyses
# ======================================================
@app.route("/logs", methods=["GET"])
def get_logs():
    """Retourne les 50 derni√®res analyses enregistr√©es."""
    logs = []
    try:
        if os.path.exists("logs.jsonl"):
            with open("logs.jsonl", "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        logs.append(json.loads(line))
                    except Exception:
                        continue
        logs = sorted(logs, key=lambda x: x.get("timestamp", ""), reverse=True)[:50]
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    return jsonify(logs)


# ======================================================
# Diagnostic / version
# ======================================================
@app.route("/version")
def version():
    return jsonify({"version": "De Facto v2.7-explicable-CSE", "status": "‚úÖ actif"})


# ======================================================
# Frontend (Replit uniquement)
# ======================================================
if os.getenv("REPL_ID"):
    @app.route("/")
    def serve_frontend():
        return send_from_directory(os.path.join(os.getcwd(), "frontend"), "index.html")

    @app.route("/<path:path>")
    def serve_static(path):
        frontend_path = os.path.join(os.getcwd(), "frontend")
        file_path = os.path.join(frontend_path, path)
        if os.path.exists(file_path):
            return send_from_directory(frontend_path, path)
        else:
            return send_from_directory(frontend_path, "index.html")


# ======================================================
# Run
# ======================================================
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 5000)))
