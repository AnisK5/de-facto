from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from openai import OpenAI
import os, signal, json, re, requests, urllib.parse, time
from datetime import datetime
from dotenv import load_dotenv

# ======================================================
# ‚öôÔ∏è Feature flags ‚Äî activables/d√©sactivables sans casser
# ======================================================
ENABLE_SYNTHESIS = True       # Ajoute une synth√®se narrative lisible
ENABLE_CONTEXT_BOX = True     # Ajoute un √©clairage contextuel court + web enrichi
ENABLE_TRANSPARENCY = True    # Ajoute mentions "exp√©rimental" et tronquage
ENABLE_URL_EXTRACT = True     # Active Trafilatura (si URL fournie)

# ======================================================
# Flask setup
# ======================================================
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}}, supports_credentials=True)

# ======================================================
# OpenAI client
# ======================================================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ======================================================
# Timeout (Render/Replit safety)
# ======================================================
def _timeout_handler(signum, frame):
    raise TimeoutError("Analyse trop longue (timeout Render).")
signal.signal(signal.SIGALRM, _timeout_handler)

# ======================================================
# Helpers
# ======================================================
def color_for(score: int) -> str:
    if score is None: return "‚ö™"
    if score >= 70: return "üü¢"
    if score >= 40: return "üü°"
    return "üî¥"

# ======================================================
# üåê Recherche Google CSE (Programmable Search API)
# ======================================================
ALLOWED_SITES = [
    "reuters.com", "apnews.com", "bbc.com",
    "lemonde.fr", "francetvinfo.fr",
    "lefigaro.fr", "liberation.fr", "leparisien.fr"
]

def search_web_results(queries, per_query=5, pause=0.5):
    """Recherche Google CSE (Programmable Search API) sur plusieurs entit√©s ou requ√™tes."""
    api_key = os.getenv("GOOGLE_CSE_API_KEY")
    cx = os.getenv("GOOGLE_CSE_CX")
    if not api_key or not cx:
        print("‚ö†Ô∏è GOOGLE_CSE_API_KEY ou GOOGLE_CSE_CX manquant ‚Äî recherche d√©sactiv√©e.")
        return []

    all_hits = []
    seen = set()
    for q in queries:
        site_filter = " OR ".join([f"site:{s}" for s in ALLOWED_SITES])
        full_q = f"{q} ({site_filter})"
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": cx,
            "q": full_q,
            "num": per_query,
            "hl": "fr",
            "lr": "lang_fr",
            "safe": "off"
        }
        try:
            r = requests.get(url, params=params, timeout=10)
            if r.status_code != 200:
                print("‚ö†Ô∏è Erreur Google CSE:", r.status_code, r.text[:200])
                continue
            data = r.json()
            results = []
            for item in data.get("items", []) or []:
                link = item.get("link")
                if link and link not in seen:
                    seen.add(link)
                    results.append({
                        "titre": item.get("title"),
                        "snippet": item.get("snippet"),
                        "url": link
                    })
            if results:
                all_hits.append({"entit√©": q, "sources": results})
            time.sleep(pause)
        except Exception as e:
            print("‚ö†Ô∏è Erreur recherche Google:", e)
            continue
    return all_hits









# ======================================================
# üß© Commentaire web
# ======================================================

def formate_commentaires_web(web_info):
    """Cr√©e un commentaire journalistique √† partir des faits manquants, contradictions et divergences."""
    commentaires = []

    # Contradictions : ton ‚Äúfact-check‚Äù nuanc√©
    for c in web_info.get("contradictions", []) or []:
        if isinstance(c, dict):
            commentaires.append(
                f"Selon {c.get('source', 'une source')}, {c.get('correction_ou_nuance', '').strip()} "
                f"ce qui nuance l‚Äôaffirmation du texte ({c.get('affirmation_du_texte', '').strip()})."
            )
        elif isinstance(c, str):
            commentaires.append(c.strip())

    # Faits manquants : ton ‚Äúanalyse critique‚Äù
    for f in web_info.get("faits_manquants", []) or []:
        if isinstance(f, dict):
            commentaires.append(
                f"Le texte n‚Äô√©voque pas {f.get('description', '').strip()} "
                f"(mentionn√© par {f.get('source', 'une autre source')}). "
                f"{f.get('explication', '').strip()}"
            )

    # Divergences de cadrage : ton ‚Äúanalyse narrative‚Äù
    for d in web_info.get("divergences_de_cadrage", []) or []:
        if isinstance(d, dict):
            commentaires.append(
                f"Le cadrage diff√®re : {d.get('resume', '').strip()} "
                f"{d.get('impact', '').strip()}"
            )

    # Synth√®se finale (courte)
    synth = web_info.get("synthese", "")
    if synth:
        commentaires.append(synth.strip())

    return " ".join(commentaires[:5]) or "Aucun √©cart majeur entre le texte et les sources consult√©es."


# ======================================================
# üß© Route principale : analyse
# ======================================================
@app.route("/analyze", methods=["POST", "OPTIONS"])
def analyze():
    if request.method == "OPTIONS":
        return ("", 204)

    payload = request.get_json(silent=True) or {}
    text = (payload.get("text") or "").strip()
    if not text:
        return jsonify({"error": "Aucun texte re√ßu"}), 400

    # üîó Extraction d‚ÄôURL via Trafilatura (si activ√©e)
    if ENABLE_URL_EXTRACT and re.match(r"^https?://", text):
        try:
            import trafilatura
            fetched = trafilatura.extract(trafilatura.fetch_url(text)) or ""
            if len(fetched.strip()) >= 300:
                text = fetched.strip()[:8000]
                print(f"‚úÖ Trafilatura OK (len={len(text)})")
            else:
                print("‚ö†Ô∏è Extraction trop courte, texte brut conserv√©.")
        except Exception as e:
            print("‚ö†Ô∏è Trafilatura indisponible :", e)

    # Tronquage protecteur
    MAX_LEN = 8000
    texte_tronque = len(text) > MAX_LEN
    original_length = len(text)
    if texte_tronque:
        text = text[:MAX_LEN] + " [...] (texte tronqu√© pour analyse)"

    # ======================================================
    # üß© √âtape 1 ‚Äî Pr√©-analyse de type de texte (faits/opinions/autres)
    # ======================================================
    try:
        pre_prompt = f"""
        Classe le texte selon 3 cat√©gories :
        - FAITS (affirmations v√©rifiables)
        - OPINIONS (jugements ou interpr√©tations)
        - AUTRES (ironie, satire, po√©sie, r√©cit, etc.)

        Retourne un JSON au format :
        {{
          "faits": <int>,
          "opinions": <int>,
          "autres": <int>
        }}
        Texte :
        {text[:2000]}
        """
        pre_resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Tu es un linguiste qui cat√©gorise les phrases d'un texte."},
                {"role": "user", "content": pre_prompt}
            ],
            temperature=0
        )
        raw_content = pre_resp.choices[0].message.content.strip()
        # Parsing JSON robuste avec regex
        try:
            fact_mix = json.loads(raw_content)
        except json.JSONDecodeError:
            # Tenter d'extraire le JSON du texte
            m = re.search(r"\{.*\}", raw_content, re.DOTALL)
            if m:
                fact_mix = json.loads(m.group(0))
            else:
                raise
    except Exception as e:
        print("‚ö†Ô∏è Erreur pr√©-analyse :", e)
        fact_mix = {"faits": 0, "opinions": 0, "autres": 0}

    total = sum(fact_mix.values()) or 1
    densite_faits = int((fact_mix["faits"] / total) * 100)
    type_texte = (
        "Principalement factuel" if densite_faits > 60 else
        "Opinion ou analyse" if fact_mix["opinions"] > 40 else
        "Autre (narratif, satirique‚Ä¶)"
    )

    # ======================================================
    # üåç √âtape interm√©diaire : Recherche Web enrichie
    #    (entit√©s ‚Üí recherche ‚Üí faits manquants / contradictions / impact)
    # ======================================================
    def web_context_research(text: str):
        """
        √âtape d'enrichissement factuel :
        1) Extrait les entit√©s du texte (personnes, lieux, orga, √©v√©nements)
        2) Recherche des sources fiables (Reuters, AP, BBC, Le Monde, Franceinfo)
        3) Synth√©tise : faits manquants pr√©cis + contradictions + impact + fiabilit√©
        Retour JSON robuste m√™me en cas d'√©chec partiel.
        """
        try:
            # 1) Extraction d'entit√©s
            ent_prompt = f"""
            Extrait les principales entit√©s nomm√©es (personnes, lieux, organisations, √©v√©nements, lois, chiffres cl√©s)
            du texte suivant :
            {text[:2000]}

            R√©ponds uniquement en JSON : ["entit√©1", "entit√©2", ...]
            """
            ent_resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu es un extracteur d'entit√©s journalistiques (NER)."},
                    {"role": "user", "content": ent_prompt}
                ],
                temperature=0
            )
            raw_entities = ent_resp.choices[0].message.content.strip()
            try:
                entities = json.loads(raw_entities)
            except json.JSONDecodeError:
                # Tenter d'extraire le JSON array du texte
                m = re.search(r"\[.*\]", raw_entities, re.DOTALL)
                if m:
                    try:
                        entities = json.loads(m.group(0))
                    except Exception:
                        entities = []
                else:
                    entities = []
            if not isinstance(entities, list):
                entities = []
            entities = [e for e in entities if isinstance(e, str) and e.strip()]
            if not entities:
                return {
                    "recherches_effectuees": [],
                    "faits_manquants": [],
                    "contradictions": [],
                    "impact": "faible",
                    "fiabilite_sources": "Aucune source consultable (pas d'entit√©s d√©tect√©es).",
                    "synthese": "Aucune entit√© d√©tect√©e ‚Äî enrichissement impossible."
                }

            # 2) Recherche Web (Google CSE) ‚Äî requ√™tes multi-angles
            queries = []
            for ent in entities[:5]:
                queries += [
                    f"{ent} actualit√©",
                    f"{ent} controverse",
                    f"{ent} critiques",
                    f"{ent} biographie",
                    f"{ent} politique"
                ]
            
            print("üåç Recherche web activ√©e ‚Äî entit√©s d√©tect√©es :", entities)
            recherches = search_web_results(queries, per_query=4)
            print("‚úÖ Recherche web termin√©e, r√©sultats trouv√©s :", len(recherches))

            # 3) Fusion IA : comparer texte vs r√©sultats
            synth_prompt = f"""
            Tu es un assistant d'analyse journalistique et de fact-checking avanc√©.
            Ta mission est d‚Äô√©valuer le texte fourni en le confrontant √† des sources d‚Äôinformation fiables du web.
            Tu dois adopter une approche nuanc√©e, capable de d√©tecter :
            - les faits compl√©mentaires,
            - les omissions,
            - les divergences de cadrage,
            - et les interpr√©tations diff√©rentes ou contraires.

            TEXTE √Ä ANALYSER :
            {text}

            SOURCES WEB :
            {json.dumps(recherches, ensure_ascii=False, indent=2)}

            Tu r√©pondras en JSON structur√©, selon le format suivant :

            {{
              "faits_manquants": [
                {{
                  "description": "D√©cris un fait, une donn√©e, un acteur ou un point de vue pertinent non mentionn√© dans le texte, mais pr√©sent dans les sources.",
                  "source": "<nom du m√©dia ou acteur>",
                  "url": "<lien vers la source>",
                  "explication": "Explique comment cette omission ou ce compl√©ment modifierait la compr√©hension du texte (ex: change l‚Äô√©quilibre, nuance une affirmation, apporte un contexte contradictoire, etc.)."
                }}
              ],
              "contradictions": [
                {{
                  "affirmation_du_texte": "Phrase, id√©e ou ton du texte √† confronter.",
                  "correction_ou_nuance": "√ânonce ce que disent les sources web (faits, citations, chiffres, etc.) qui contredisent ou relativisent l'affirmation.",
                  "source": "<m√©dia ou acteur>",
                  "url": "<lien>"
                }}
              ],
              "divergences_de_cadrage": [
                {{
                  "resume": "D√©cris un √©cart d'angle, de ton ou de narration entre le texte et les sources (par ex : l‚Äôarticle met l‚Äôaccent sur X alors que les sources insistent sur Y).",
                  "impact": "Explique en quoi ce cadrage diff√©rent influence la perception du lecteur."
                }}
              ],
              "impact_global": "<faible|moyen|fort>",
              "fiabilite_sources": "D√©cris bri√®vement la cr√©dibilit√©, diversit√© et coh√©rence des sources trouv√©es.",
              "synthese": "R√©dige une synth√®se fluide (3‚Äì6 phrases) qui explique comment le texte se positionne par rapport aux faits √©tablis et aux autres r√©cits du web. Sois analytique, nuanc√© et journalistique ‚Äî ni moralisateur ni m√©canique."
            }}

            R√®gles de style :
            - Adopte un ton journalistique neutre, comme dans une rubrique de fact-checking du Monde, Reuters ou AFP.
            - √âvite les jugements (‚Äúfaux‚Äù, ‚Äúmensonger‚Äù) sauf si la contradiction est flagrante.
            - Sois capable d‚Äôint√©grer plusieurs angles (scientifique, politique, social) selon le sujet.
            - Si les sources ne permettent pas de confirmer ni d‚Äôinfirmer, dis-le explicitement.
            - Ne dupliques pas les extraits ; reformule clairement.
            """


            synth_resp = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Tu es un fact-checker journalistique expert et neutre."},
                    {"role": "user", "content": synth_prompt}
                ],
                temperature=0.3
            )

            content = synth_resp.choices[0].message.content.strip()
            try:
                web_summary = json.loads(content)
            except Exception:
                m = re.search(r"\{.*\}", content, re.DOTALL)
                web_summary = json.loads(m.group(0)) if m else {
                    "faits_manquants": [],
                    "contradictions": [],
                    "impact": "faible",
                    "fiabilite_sources": "R√©ponse non structur√©e.",
                    "synthese": "Le mod√®le n‚Äôa pas pu formater correctement la r√©ponse."
                }

            return web_summary


            try:
                result = json.loads(synth_resp.choices[0].message.content.strip())
            except Exception:
                # fallback compact si parsing impossible
                result = {
                    "faits_manquants": [],
                    "contradictions": [],
                    "impact": "faible",
                    "fiabilite_sources": "Synth√®se non structur√©e.",
                    "synthese": "La synth√®se n'a pas pu √™tre structur√©e."
                }

            # joindre les recherches brutes pour le front / debug
            result["recherches_effectuees"] = recherches
            # normaliser impact
            impact = (result.get("impact") or "faible").strip().lower()
            if impact not in ("faible", "moyen", "fort"):
                impact = "faible"
            result["impact"] = impact

            # nettoyer faits_manquants (format stable)
            fm = []
            for f in result.get("faits_manquants", []) or []:
                if isinstance(f, dict) and f.get("texte"):
                    fm.append({
                        "texte": str(f.get("texte")).strip(),
                        "source": (f.get("source") or "").strip() or None,
                        "url": (f.get("url") or "").strip() or None
                    })
            result["faits_manquants"] = fm

            # contradictions => liste de str
            contr = []
            for c in result.get("contradictions", []) or []:
                if isinstance(c, str) and c.strip():
                    contr.append(c.strip())
            result["contradictions"] = contr

            return result

        except Exception as e:
            print("‚ö†Ô∏è Web context failed:", e)
            return {
                "recherches_effectuees": [],
                "faits_manquants": [],
                "contradictions": [],
                "impact": "faible",
                "fiabilite_sources": "Recherche contextuelle non disponible.",
                "synthese": "Recherche contextuelle non disponible."
            }

    web_info = web_context_research(text) if ENABLE_CONTEXT_BOX else {
        "recherches_effectuees": [],
        "faits_manquants": [],
        "contradictions": [],
        "impact": "faible",
        "fiabilite_sources": "Contexte non activ√©.",
        "synthese": "Contexte non activ√©."
    }

    # ======================================================
    # üß† √âtape 3 ‚Äî Analyse principale compl√®te
    # ======================================================
    prompt = f"""
    Tu es **De Facto**, un analyste journalistique sp√©cialis√© dans la compr√©hension critique de l'information.  
    Ta mission : aider un lecteur √† **comprendre ce qu‚Äôun texte dit, omet et sous-entend**, en t‚Äôappuyant sur les faits v√©rifiables et les sources web.

    ---

    ### üéØ Objectif g√©n√©ral
    Fournir une **analyse journalistique claire, utile et engageante**, qui r√©v√®le ce que l‚Äôutilisateur aurait *int√©r√™t √† savoir* apr√®s avoir lu cet article.

    Tu dois produire :
    - des **sous-notes riches et explicatives** (justesse, compl√©tude, ton, sophismes)
    - une **synth√®se fluide et percutante**, r√©dig√©e dans le style d‚Äôun article bref de fact-checking.

    ---

    ### üß© Structure de sortie (STRICT JSON)
    R√©ponds uniquement en JSON au format suivant :
    {{
      "score_global": <int>,
      "couleur_global": "<emoji>",
      "axes": {{
        "fond": {{
          "justesse": {{
            "note": <int>,
            "couleur": "<emoji>",
            "justification": "<2‚Äì3 phrases d‚Äôanalyse pr√©cises, avec exemples>",
            "citation": "<<=20 mots ou null>"
          }},
          "completude": {{
            "note": <int>,
            "couleur": "<emoji>",
            "justification": "<2‚Äì3 phrases sur les omissions ou contrepoints manquants>",
            "citation": "<<=20 mots ou null>"
          }}
        }},
        "forme": {{
          "ton": {{
            "note": <int>,
            "couleur": "<emoji>",
            "justification": "<2‚Äì3 phrases sur le ton, les choix lexicaux, l‚Äôorientation implicite>",
            "citation": "<<=20 mots ou null>"
          }},
          "sophismes": {{
            "note": <int>,
            "couleur": "<emoji>",
            "justification": "<2‚Äì3 phrases sur la rigueur argumentative, ou absence de sophismes>",
            "citation": "<<=20 mots ou null>"
          }}
        }}
      }},
      "commentaire": "<2 phrases fortes, journalistiques, synth√©tisant le verdict global>",
      "resume": "<3‚Äì6 phrases de synth√®se fluide et captivante>",
      "confiance_analyse": <int>,
      "explication_confiance": "<phrase sur la fiabilit√© de l‚Äôanalyse>",
      "hypothese_interpretative": "<1 phrase sur la logique m√©diatique ou √©ditoriale du texte>",
      "limites_analyse_ia": ["<texte>", "..."],
      "limites_analyse_contenu": ["<texte>", "..."],
      "recherches_effectuees": ["<r√©sum√© court>", "..."],
      "methode": {{
        "principe": "De Facto √©value un texte selon deux axes : FOND (justesse, compl√©tude) et FORME (ton, sophismes).",
        "criteres": {{
          "fond": "Justesse (v√©racit√©, rigueur, v√©rification) et compl√©tude (angles et voix absentes).",
          "forme": "Ton (neutralit√© ou orientation implicite) et sophismes (raisonnements biais√©s)."
        }},
        "avertissement": "Analyse exp√©rimentale ‚Äî l‚ÄôIA peut commettre des erreurs."
      }}
    }}

    ---

    ### üß† Directives d√©taill√©es pour chaque axe

    #### üîπ Justesse
    - V√©rifie les faits, dates, chiffres et citations avec les sources web.
    - Si des contradictions existent, mentionne-les clairement.
    - Reformule les √©carts sans jugement moral (‚Äúinexact‚Äù, ‚Äúerron√©‚Äù), mais avec pr√©cision journalistique.
    - Exemple :  
      > ¬´ L‚Äôarticle √©voque la nomination de Revel mais omet sa p√©riode √† la CNAM, pourtant confirm√©e par Le Monde. ¬ª

    #### üîπ Compl√©tude
    - Rep√®re les points de vue, r√©actions ou contextes absents du texte.
    - Utilise les donn√©es du web pour citer les √©l√©ments manquants.
    - Explique **en quoi ces absences modifient la perception** du lecteur.
    - Exemple :  
      > ¬´ Le texte ignore les critiques de l‚Äôopposition, ce qui donne l‚Äôimpression d‚Äôun consensus. ¬ª

    #### üîπ Ton
    - Analyse le choix des mots, adjectifs, formules, citations.
    - D√©termine s‚Äôil existe une orientation implicite (valorisante, ironique, dramatique‚Ä¶).
    - Exemple :  
      > ¬´ L‚Äôexpression ‚Äúprofil technique‚Äù v√©hicule une image valorisante et apaise la dimension politique. ¬ª

    #### üîπ Sophismes
    - √âvalue la rigueur du raisonnement.
    - D√©cris s‚Äôil y a des g√©n√©ralisations abusives, corr√©lations h√¢tives ou simplifications.
    - Si le texte est solide, √©cris-le aussi clairement.
    - Exemple :  
      > ¬´ L‚Äôauteur confond corr√©lation et causalit√© en reliant la popularit√© au seul changement de Premier ministre. ¬ª

    Chaque justification doit contenir **2 √† 3 phrases denses**, avec au moins un exemple concret ou citation courte du texte.

    ---

    ### üì∞ Synth√®se finale (commentaire + r√©sum√©)
    - Le **commentaire** (2 phrases) donne le **verdict clair et journalistique**.  
      > ‚ÄúGlobalement cr√©dible mais partiel : l‚Äôarticle informe correctement mais n√©glige certains aspects cl√©s.‚Äù
    - Le **r√©sum√©** (3 √† 6 phrases) raconte l‚Äôhistoire du texte :
      - Ce que le texte dit,  
      - Ce qu‚Äôil omet,  
      - Ce que d‚Äôautres sources r√©v√®lent,  
      - Ce que cela change dans la perception du lecteur.

    Le ton doit √™tre fluide, professionnel et engageant ‚Äî comme une courte chronique de m√©diation de l‚Äôinfo.

    ---

    ### üåç Compl√©ments factuels issus du Web
    Voici les informations disponibles issues de la recherche contextuelle :
    {json.dumps(web_info, ensure_ascii=False, indent=2)}

    Utilise ces donn√©es pour :
    - confirmer ou nuancer la justesse,
    - identifier les faits manquants ou points de vue oppos√©s,
    - renforcer la cr√©dibilit√© de ton analyse.

    ---

    ### ‚öîÔ∏è Instructions finales
    1. Structure ta r√©ponse uniquement en JSON (pas de texte libre autour).
    2. Appuie-toi sur les √©l√©ments concrets du texte et des sources web.
    3. √âvite les formulations vagues (‚Äúle texte manque de d√©tails‚Äù).
    4. Ta mission est de **r√©v√©ler ce que le lecteur n‚Äôavait pas per√ßu** ‚Äî le rendre plus lucide.

    ---

    ### üßæ Texte √† analyser :
    {text}
    """


    try:
        signal.alarm(45)
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Tu es un analyste journalistique rigoureux, concis et clair."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.25
        )
        signal.alarm(0)

        raw = resp.choices[0].message.content.strip()
        try:
            result = json.loads(raw)
        except json.JSONDecodeError:
            m = re.search(r"\{.*\}", raw, re.DOTALL)
            if not m:
                return jsonify({"error": "R√©ponse GPT non conforme (non JSON)"}), 500
            result = json.loads(m.group(0))

        # ======================================================
        # Post-traitement enrichi
        # ======================================================
        result.setdefault("confiance_analyse", 80)
        result.setdefault("type_texte", type_texte)
        result.setdefault("densite_faits", densite_faits)

        # üéØ Pond√©ration douce du score global selon densit√© factuelle
        if "score_global" in result:
            sg = int(result["score_global"])
            if densite_faits > 60:
                sg = min(sg + 5, 100)
            elif densite_faits < 30:
                sg = max(sg - 5, 0)
            result["score_global"] = sg
            result["couleur_global"] = color_for(sg)

        # üîé Ajoute info sur le contenu du texte
        result["composition"] = {
            "faits": fact_mix["faits"],
            "opinions": fact_mix["opinions"],
            "autres": fact_mix["autres"],
            "densite_faits": densite_faits
        }

        # üåç Ins√®re le contexte Web brute pour le front
        result["faits_complementaires"] = web_info.get("faits_manquants", [])
        result["contexte_synthese"] = web_info.get("synthese")
        result["contexte_impact"] = web_info.get("impact")
        result["contexte_contradictions"] = web_info.get("contradictions", [])
        result["contexte_fiabilite_sources"] = web_info.get("fiabilite_sources", "")
        result["recherches_effectuees"] = web_info.get("recherches_effectuees", [])

        # üßÆ POND√âRATION INTELLIGENTE SELON LE CONTEXTE WEB
        # Bar√®me explicite :
        # - Contradictions : -20 (‚â•2), -10 (1)
        # - Faits manquants : -25 (‚â•3), -15 (2), -8 (1) sur compl√©tude
        # - Impact global "moyen" : -5 sur score global ; "fort" : -10
        axes = result.get("axes", {}) or {}
        fond = axes.get("fond", {}) or {}
        justesse = (fond.get("justesse", {}) or {}).get("note", 70)
        completude = (fond.get("completude", {}) or {}).get("note", 70)

        nb_contrad = len(web_info.get("contradictions", []) or [])
        nb_faits = len(web_info.get("faits_manquants", []) or [])
        impact = (web_info.get("impact") or "faible").lower()

        # Ajustements justesse par contradictions
        if nb_contrad >= 2:
            justesse -= 20
        elif nb_contrad == 1:
            justesse -= 10

        # Ajustements compl√©tude par faits manquants
        if nb_faits >= 3:
            completude -= 25
        elif nb_faits == 2:
            completude -= 15
        elif nb_faits == 1:
            completude -= 8

        # Clamps 0..100
        justesse = max(0, min(100, int(justesse)))
        completude = max(0, min(100, int(completude)))

        # Replace in result if structure exists
        if "fond" in axes:
            if "justesse" in axes["fond"]:
                axes["fond"]["justesse"]["note"] = justesse
                axes["fond"]["justesse"]["couleur"] = color_for(justesse)
            if "completude" in axes["fond"]:
                axes["fond"]["completude"]["note"] = completude
                axes["fond"]["completude"]["couleur"] = color_for(completude)

        # Ajustement score global par impact
        if "score_global" in result:
            if "fort" in impact:
                result["score_global"] = max(0, result["score_global"] - 10)
            elif "moyen" in impact:
                result["score_global"] = max(0, result["score_global"] - 5)
            result["couleur_global"] = color_for(result["score_global"])

        # ‚úÖ (Optionnel) Enregistrer une trace pour /logs
        try:
            log_item = {
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "input_len": original_length,
                "texte_tronque": bool(texte_tronque),
                "type_texte": type_texte,
                "densite_faits": densite_faits,
                "web_faits_manquants": nb_faits,
                "web_contradictions": nb_contrad,
                "web_impact": impact,
                "score_global": result.get("score_global"),
                "axes": result.get("axes", {}),
                "resume": result.get("resume"),
                "commentaire": result.get("commentaire"),
            }
            with open("logs.jsonl", "a", encoding="utf-8") as f:
                f.write(json.dumps(log_item, ensure_ascii=False) + "\n")
        except Exception as e:
            print("‚ÑπÔ∏è √âchec d'√©criture logs.jsonl :", e)

        # üîç Int√©gration narrative du contexte web
        commentaire_web = formate_commentaires_web(web_info)
        if "commentaire" in result and isinstance(result["commentaire"], str):
            result["commentaire"] += " " + commentaire_web
        else:
            result["commentaire"] = commentaire_web

        # Bonus : renforce le r√©sum√© avec la synth√®se web si disponible
        if web_info.get("synthese"):
            if "resume" in result and isinstance(result["resume"], str):
                result["resume"] += " " + web_info["synthese"]
            else:
                result["resume"] = web_info["synthese"]

        
        print("üß† Synth√®se web contextuelle :", json.dumps(web_info, ensure_ascii=False, indent=2))
        
        return jsonify(result)

    except TimeoutError:
        return jsonify({"error": "Analyse trop longue. R√©essaie avec un texte plus court."}), 500
    except Exception as e:
        print("‚ùå Erreur :", e)
        return jsonify({"error": str(e)}), 500


# ======================================================
# üìú Historique des analyses
# ======================================================
@app.route("/logs", methods=["GET"])
def get_logs():
    """Retourne les 50 derni√®res analyses enregistr√©es."""
    logs = []
    try:
        if os.path.exists("logs.jsonl"):
            with open("logs.jsonl", "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        logs.append(json.loads(line))
                    except Exception:
                        continue
        logs = sorted(logs, key=lambda x: x.get("timestamp", ""), reverse=True)[:50]
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    return jsonify(logs)


# ======================================================
# Diagnostic / version
# ======================================================
@app.route("/version")
def version():
    return jsonify({"version": "De Facto v2.7-explicable-CSE", "status": "‚úÖ actif"})


# ======================================================
# Frontend (Replit uniquement)
# ======================================================
if os.getenv("REPL_ID"):
    @app.route("/")
    def serve_frontend():
        return send_from_directory(os.path.join(os.getcwd(), "frontend"), "index.html")

    @app.route("/<path:path>")
    def serve_static(path):
        frontend_path = os.path.join(os.getcwd(), "frontend")
        file_path = os.path.join(frontend_path, path)
        if os.path.exists(file_path):
            return send_from_directory(frontend_path, path)
        else:
            return send_from_directory(frontend_path, "index.html")


# ======================================================
# Run
# ======================================================
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 5000)))
